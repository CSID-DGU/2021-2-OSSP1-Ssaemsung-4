<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.2" />
<title>uisrnn.utils API documentation</title>
<meta name="description" content="Utils for UIS-RNN." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}#index .two-column{column-count:2}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>uisrnn.utils</code> module</h1>
</header>
<section id="section-intro">
<p>Utils for UIS-RNN.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python"># Copyright 2018 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &#34;AS IS&#34; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
&#34;&#34;&#34;Utils for UIS-RNN.&#34;&#34;&#34;

import random
import string

import numpy as np
import torch
from torch import autograd


class Logger:
  &#34;&#34;&#34;A class for printing logging information to screen.&#34;&#34;&#34;

  def __init__(self, verbosity):
    self._verbosity = verbosity

  def print(self, level, message):
    &#34;&#34;&#34;Print a message if level is not higher than verbosity.

    Args:
      level: the level of this message, smaller value means more important
      message: the message to be printed
    &#34;&#34;&#34;
    if level &lt;= self._verbosity:
      print(message)


def generate_random_string(length=6):
  &#34;&#34;&#34;Generate a random string of upper case letters and digits.

  Args:
    length: length of the generated string

  Returns:
    the generated string
  &#34;&#34;&#34;
  return &#39;&#39;.join([
      random.choice(string.ascii_uppercase + string.digits)
      for _ in range(length)])


def enforce_cluster_id_uniqueness(cluster_ids):
  &#34;&#34;&#34;Enforce uniqueness of cluster id across sequences.

  Args:
    cluster_ids: a list of 1-dim list/numpy.ndarray of strings

  Returns:
    a new list with same length of cluster_ids

  Raises:
    TypeError: if cluster_ids or its element has wrong type
  &#34;&#34;&#34;
  if not isinstance(cluster_ids, list):
    raise TypeError(&#39;cluster_ids must be a list&#39;)
  new_cluster_ids = []
  for cluster_id in cluster_ids:
    sequence_id = generate_random_string()
    if isinstance(cluster_id, np.ndarray):
      cluster_id = cluster_id.tolist()
    if not isinstance(cluster_id, list):
      raise TypeError(&#39;Elements of cluster_ids must be list or numpy.ndarray&#39;)
    new_cluster_id = [&#39;_&#39;.join([sequence_id, s]) for s in cluster_id]
    new_cluster_ids.append(new_cluster_id)
  return new_cluster_ids


def concatenate_training_data(train_sequences, train_cluster_ids,
                              enforce_uniqueness=True, shuffle=True):
  &#34;&#34;&#34;Concatenate training data.

  Args:
    train_sequences: a list of 2-dim numpy arrays to be concatenated
    train_cluster_ids: a list of 1-dim list/numpy.ndarray of strings
    enforce_uniqueness: a boolean indicated whether we should enfore uniqueness
      to train_cluster_ids
    shuffle: whether to randomly shuffle input order

  Returns:
    concatenated_train_sequence: a 2-dim numpy array
    concatenated_train_cluster_id: a list of strings

  Raises:
    TypeError: if input has wrong type
    ValueError: if sizes/dimensions of input or their elements are incorrect
  &#34;&#34;&#34;
  # check input
  if not isinstance(train_sequences, list) or not isinstance(
      train_cluster_ids, list):
    raise TypeError(&#39;train_sequences and train_cluster_ids must be lists&#39;)
  if len(train_sequences) != len(train_cluster_ids):
    raise ValueError(
        &#39;train_sequences and train_cluster_ids must have same size&#39;)
  train_cluster_ids = [
      x.tolist() if isinstance(x, np.ndarray) else x
      for x in train_cluster_ids]
  global_observation_dim = None
  for i, (train_sequence, train_cluster_id) in enumerate(
      zip(train_sequences, train_cluster_ids)):
    train_length, observation_dim = train_sequence.shape
    if i == 0:
      global_observation_dim = observation_dim
    elif global_observation_dim != observation_dim:
      raise ValueError(
          &#39;train_sequences must have consistent observation dimension&#39;)
    if not isinstance(train_cluster_id, list):
      raise TypeError(
          &#39;Elements of train_cluster_ids must be list or numpy.ndarray&#39;)
    if len(train_cluster_id) != train_length:
      raise ValueError(
          &#39;Each train_sequence and its train_cluster_id must have same length&#39;)

  # enforce uniqueness
  if enforce_uniqueness:
    train_cluster_ids = enforce_cluster_id_uniqueness(train_cluster_ids)

  # random shuffle
  if shuffle:
    zipped_input = list(zip(train_sequences, train_cluster_ids))
    random.shuffle(zipped_input)
    train_sequences, train_cluster_ids = zip(*zipped_input)

  # concatenate
  concatenated_train_sequence = np.concatenate(train_sequences, axis=0)
  concatenated_train_cluster_id = [x for train_cluster_id in train_cluster_ids
                                   for x in train_cluster_id]
  return concatenated_train_sequence, concatenated_train_cluster_id


def sample_permuted_segments(index_sequence, number_samples):
  &#34;&#34;&#34;Sample sequences with permuted blocks.

  Args:
    index_sequence: (integer array, size: L)
      - subsequence index
      For example, index_sequence = [1,2,6,10,11,12].
    number_samples: (integer)
      - number of subsampled block-preserving permuted sequences.
      For example, number_samples = 5

  Returns:
    sampled_index_sequences: (a list of numpy arrays) - a list of subsampled
      block-preserving permuted sequences. For example,
    ```
    sampled_index_sequences =
    [[10,11,12,1,2,6],
     [6,1,2,10,11,12],
     [1,2,10,11,12,6],
     [6,1,2,10,11,12],
     [1,2,6,10,11,12]]
    ```
      The length of &#34;sampled_index_sequences&#34; is &#34;number_samples&#34;.
  &#34;&#34;&#34;
  segments = []
  if len(index_sequence) == 1:
    segments.append(index_sequence)
  else:
    prev = 0
    for i in range(len(index_sequence) - 1):
      if index_sequence[i + 1] != index_sequence[i] + 1:
        segments.append(index_sequence[prev:(i + 1)])
        prev = i + 1
      if i + 1 == len(index_sequence) - 1:
        segments.append(index_sequence[prev:])
  # sample permutations
  sampled_index_sequences = []
  for _ in range(number_samples):
    segments_array = []
    permutation = np.random.permutation(len(segments))
    for permutation_item in permutation:
      segments_array.append(segments[permutation_item])
    sampled_index_sequences.append(np.concatenate(segments_array))
  return sampled_index_sequences


def resize_sequence(sequence, cluster_id, num_permutations=None):
  &#34;&#34;&#34;Resize sequences for packing and batching.

  Args:
    sequence: (real numpy matrix, size: seq_len*obs_size) - observed sequence
    cluster_id: (numpy vector, size: seq_len) - cluster indicator sequence
    num_permutations: int - Number of permutations per utterance sampled.

  Returns:
    sub_sequences: A list of numpy array, with obsevation vector from the same
      cluster in the same list.
    seq_lengths: The length of each cluster (+1).
    bias: Flipping coin head probability.
    bias_denominator: The denominator of the bias, used for multiple calls to
      fit().
  &#34;&#34;&#34;
  # merge sub-sequences that belong to a single cluster to a single sequence
  unique_id = np.unique(cluster_id)
  sub_sequences = []
  seq_lengths = []
  if num_permutations and num_permutations &gt; 1:
    for i in unique_id:
      idx_set = np.where(cluster_id == i)[0]
      sampled_idx_sets = sample_permuted_segments(idx_set, num_permutations)
      for j in range(num_permutations):
        sub_sequences.append(sequence[sampled_idx_sets[j], :])
        seq_lengths.append(len(idx_set) + 1)
  else:
    for i in unique_id:
      idx_set = np.where(cluster_id == i)
      sub_sequences.append(sequence[idx_set, :][0])
      seq_lengths.append(len(idx_set[0]) + 1)

  # compute bias
  transit_num = 0
  for entry in range(len(cluster_id) - 1):
    transit_num += (cluster_id[entry] != cluster_id[entry + 1])
  bias_denominator = len(cluster_id)
  bias = (transit_num + 1) / bias_denominator
  return sub_sequences, seq_lengths, bias, bias_denominator


def pack_sequence(
    sub_sequences, seq_lengths, batch_size, observation_dim, device):
  &#34;&#34;&#34;Pack sequences for training.

  Args:
    sub_sequences: A list of numpy array, with obsevation vector from the same
      cluster in the same list.
    seq_lengths: The length of each cluster (+1).
    batch_size: int or None - Run batch learning if batch_size is None. Else,
      run online learning with specified batch size.
    observation_dim: int - dimension for observation vectors
    device: str - Your device. E.g., `cuda:0` or `cpu`.

  Returns:
    packed_rnn_input: (PackedSequence object) packed rnn input
    rnn_truth: ground truth
  &#34;&#34;&#34;
  num_clusters = len(seq_lengths)
  sorted_seq_lengths = np.sort(seq_lengths)[::-1]
  permute_index = np.argsort(seq_lengths)[::-1]

  if batch_size is None:
    rnn_input = np.zeros((sorted_seq_lengths[0],
                          num_clusters,
                          observation_dim))
    for i in range(num_clusters):
      rnn_input[1:sorted_seq_lengths[i], i,
                :] = sub_sequences[permute_index[i]]
    rnn_input = autograd.Variable(
        torch.from_numpy(rnn_input).float()).to(device)
    packed_rnn_input = torch.nn.utils.rnn.pack_padded_sequence(
        rnn_input, sorted_seq_lengths, batch_first=False)
  else:
    mini_batch = np.sort(np.random.choice(num_clusters, batch_size))
    rnn_input = np.zeros((sorted_seq_lengths[mini_batch[0]],
                          batch_size,
                          observation_dim))
    for i in range(batch_size):
      rnn_input[1:sorted_seq_lengths[mini_batch[i]],
                i, :] = sub_sequences[permute_index[mini_batch[i]]]
    rnn_input = autograd.Variable(
        torch.from_numpy(rnn_input).float()).to(device)
    packed_rnn_input = torch.nn.utils.rnn.pack_padded_sequence(
        rnn_input, sorted_seq_lengths[mini_batch], batch_first=False)
  # ground truth is the shifted input
  rnn_truth = rnn_input[1:, :, :]
  return packed_rnn_input, rnn_truth


def output_result(model_args, training_args, test_record):
  &#34;&#34;&#34;Produce a string to summarize the experiment.&#34;&#34;&#34;
  accuracy_array, _ = zip(*test_record)
  total_accuracy = np.mean(accuracy_array)
  output_string = &#34;&#34;&#34;
Config:
  sigma_alpha: {}
  sigma_beta: {}
  crp_alpha: {}
  learning rate: {}
  learning rate half life: {}
  regularization: {}
  batch size: {}

Performance:
  averaged accuracy: {:.6f}
  accuracy numbers for all testing sequences:
  &#34;&#34;&#34;.strip().format(
      training_args.sigma_alpha,
      training_args.sigma_beta,
      model_args.crp_alpha,
      training_args.learning_rate,
      training_args.learning_rate_half_life,
      training_args.regularization_weight,
      training_args.batch_size,
      total_accuracy)
  for accuracy in accuracy_array:
    output_string += &#39;\n    {:.6f}&#39;.format(accuracy)
  output_string += &#39;\n&#39; + &#39;=&#39; * 80 + &#39;\n&#39;
  filename = &#39;layer_{}_{}_{:.1f}_result.txt&#39;.format(
      model_args.rnn_hidden_size,
      model_args.rnn_depth, model_args.rnn_dropout)
  with open(filename, &#39;a&#39;) as file_object:
    file_object.write(output_string)
  return output_string</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="uisrnn.utils.concatenate_training_data"><code class="name flex">
<span>def <span class="ident">concatenate_training_data</span></span>(<span>train_sequences, train_cluster_ids, enforce_uniqueness=True, shuffle=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Concatenate training data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>train_sequences</code></strong></dt>
<dd>a list of 2-dim numpy arrays to be concatenated</dd>
<dt><strong><code>train_cluster_ids</code></strong></dt>
<dd>a list of 1-dim list/numpy.ndarray of strings</dd>
<dt><strong><code>enforce_uniqueness</code></strong></dt>
<dd>a boolean indicated whether we should enfore uniqueness
to train_cluster_ids</dd>
<dt><strong><code>shuffle</code></strong></dt>
<dd>whether to randomly shuffle input order</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>concatenated_train_sequence</code></strong></dt>
<dd>a 2-dim numpy array</dd>
<dt><strong><code>concatenated_train_cluster_id</code></strong></dt>
<dd>a list of strings</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>TypeError</code></strong></dt>
<dd>if input has wrong type</dd>
<dt><strong><code>ValueError</code></strong></dt>
<dd>if sizes/dimensions of input or their elements are incorrect</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def concatenate_training_data(train_sequences, train_cluster_ids,
                              enforce_uniqueness=True, shuffle=True):
  &#34;&#34;&#34;Concatenate training data.

  Args:
    train_sequences: a list of 2-dim numpy arrays to be concatenated
    train_cluster_ids: a list of 1-dim list/numpy.ndarray of strings
    enforce_uniqueness: a boolean indicated whether we should enfore uniqueness
      to train_cluster_ids
    shuffle: whether to randomly shuffle input order

  Returns:
    concatenated_train_sequence: a 2-dim numpy array
    concatenated_train_cluster_id: a list of strings

  Raises:
    TypeError: if input has wrong type
    ValueError: if sizes/dimensions of input or their elements are incorrect
  &#34;&#34;&#34;
  # check input
  if not isinstance(train_sequences, list) or not isinstance(
      train_cluster_ids, list):
    raise TypeError(&#39;train_sequences and train_cluster_ids must be lists&#39;)
  if len(train_sequences) != len(train_cluster_ids):
    raise ValueError(
        &#39;train_sequences and train_cluster_ids must have same size&#39;)
  train_cluster_ids = [
      x.tolist() if isinstance(x, np.ndarray) else x
      for x in train_cluster_ids]
  global_observation_dim = None
  for i, (train_sequence, train_cluster_id) in enumerate(
      zip(train_sequences, train_cluster_ids)):
    train_length, observation_dim = train_sequence.shape
    if i == 0:
      global_observation_dim = observation_dim
    elif global_observation_dim != observation_dim:
      raise ValueError(
          &#39;train_sequences must have consistent observation dimension&#39;)
    if not isinstance(train_cluster_id, list):
      raise TypeError(
          &#39;Elements of train_cluster_ids must be list or numpy.ndarray&#39;)
    if len(train_cluster_id) != train_length:
      raise ValueError(
          &#39;Each train_sequence and its train_cluster_id must have same length&#39;)

  # enforce uniqueness
  if enforce_uniqueness:
    train_cluster_ids = enforce_cluster_id_uniqueness(train_cluster_ids)

  # random shuffle
  if shuffle:
    zipped_input = list(zip(train_sequences, train_cluster_ids))
    random.shuffle(zipped_input)
    train_sequences, train_cluster_ids = zip(*zipped_input)

  # concatenate
  concatenated_train_sequence = np.concatenate(train_sequences, axis=0)
  concatenated_train_cluster_id = [x for train_cluster_id in train_cluster_ids
                                   for x in train_cluster_id]
  return concatenated_train_sequence, concatenated_train_cluster_id</code></pre>
</details>
</dd>
<dt id="uisrnn.utils.enforce_cluster_id_uniqueness"><code class="name flex">
<span>def <span class="ident">enforce_cluster_id_uniqueness</span></span>(<span>cluster_ids)</span>
</code></dt>
<dd>
<section class="desc"><p>Enforce uniqueness of cluster id across sequences.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>cluster_ids</code></strong></dt>
<dd>a list of 1-dim list/numpy.ndarray of strings</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>a new list with same length of cluster_ids</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><strong><code>TypeError</code></strong></dt>
<dd>if cluster_ids or its element has wrong type</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def enforce_cluster_id_uniqueness(cluster_ids):
  &#34;&#34;&#34;Enforce uniqueness of cluster id across sequences.

  Args:
    cluster_ids: a list of 1-dim list/numpy.ndarray of strings

  Returns:
    a new list with same length of cluster_ids

  Raises:
    TypeError: if cluster_ids or its element has wrong type
  &#34;&#34;&#34;
  if not isinstance(cluster_ids, list):
    raise TypeError(&#39;cluster_ids must be a list&#39;)
  new_cluster_ids = []
  for cluster_id in cluster_ids:
    sequence_id = generate_random_string()
    if isinstance(cluster_id, np.ndarray):
      cluster_id = cluster_id.tolist()
    if not isinstance(cluster_id, list):
      raise TypeError(&#39;Elements of cluster_ids must be list or numpy.ndarray&#39;)
    new_cluster_id = [&#39;_&#39;.join([sequence_id, s]) for s in cluster_id]
    new_cluster_ids.append(new_cluster_id)
  return new_cluster_ids</code></pre>
</details>
</dd>
<dt id="uisrnn.utils.generate_random_string"><code class="name flex">
<span>def <span class="ident">generate_random_string</span></span>(<span>length=6)</span>
</code></dt>
<dd>
<section class="desc"><p>Generate a random string of upper case letters and digits.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>length</code></strong></dt>
<dd>length of the generated string</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>the generated string</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def generate_random_string(length=6):
  &#34;&#34;&#34;Generate a random string of upper case letters and digits.

  Args:
    length: length of the generated string

  Returns:
    the generated string
  &#34;&#34;&#34;
  return &#39;&#39;.join([
      random.choice(string.ascii_uppercase + string.digits)
      for _ in range(length)])</code></pre>
</details>
</dd>
<dt id="uisrnn.utils.output_result"><code class="name flex">
<span>def <span class="ident">output_result</span></span>(<span>model_args, training_args, test_record)</span>
</code></dt>
<dd>
<section class="desc"><p>Produce a string to summarize the experiment.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def output_result(model_args, training_args, test_record):
  &#34;&#34;&#34;Produce a string to summarize the experiment.&#34;&#34;&#34;
  accuracy_array, _ = zip(*test_record)
  total_accuracy = np.mean(accuracy_array)
  output_string = &#34;&#34;&#34;
Config:
  sigma_alpha: {}
  sigma_beta: {}
  crp_alpha: {}
  learning rate: {}
  learning rate half life: {}
  regularization: {}
  batch size: {}

Performance:
  averaged accuracy: {:.6f}
  accuracy numbers for all testing sequences:
  &#34;&#34;&#34;.strip().format(
      training_args.sigma_alpha,
      training_args.sigma_beta,
      model_args.crp_alpha,
      training_args.learning_rate,
      training_args.learning_rate_half_life,
      training_args.regularization_weight,
      training_args.batch_size,
      total_accuracy)
  for accuracy in accuracy_array:
    output_string += &#39;\n    {:.6f}&#39;.format(accuracy)
  output_string += &#39;\n&#39; + &#39;=&#39; * 80 + &#39;\n&#39;
  filename = &#39;layer_{}_{}_{:.1f}_result.txt&#39;.format(
      model_args.rnn_hidden_size,
      model_args.rnn_depth, model_args.rnn_dropout)
  with open(filename, &#39;a&#39;) as file_object:
    file_object.write(output_string)
  return output_string</code></pre>
</details>
</dd>
<dt id="uisrnn.utils.pack_sequence"><code class="name flex">
<span>def <span class="ident">pack_sequence</span></span>(<span>sub_sequences, seq_lengths, batch_size, observation_dim, device)</span>
</code></dt>
<dd>
<section class="desc"><p>Pack sequences for training.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sub_sequences</code></strong></dt>
<dd>A list of numpy array, with obsevation vector from the same
cluster in the same list.</dd>
<dt><strong><code>seq_lengths</code></strong></dt>
<dd>The length of each cluster (+1).</dd>
<dt><strong><code>batch_size</code></strong></dt>
<dd>int or None - Run batch learning if batch_size is None. Else,
run online learning with specified batch size.</dd>
<dt><strong><code>observation_dim</code></strong></dt>
<dd>int - dimension for observation vectors</dd>
<dt><strong><code>device</code></strong></dt>
<dd>str - Your device. E.g., <code>cuda:0</code> or <code>cpu</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>packed_rnn_input</code></strong></dt>
<dd>(PackedSequence object) packed rnn input</dd>
<dt><strong><code>rnn_truth</code></strong></dt>
<dd>ground truth</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def pack_sequence(
    sub_sequences, seq_lengths, batch_size, observation_dim, device):
  &#34;&#34;&#34;Pack sequences for training.

  Args:
    sub_sequences: A list of numpy array, with obsevation vector from the same
      cluster in the same list.
    seq_lengths: The length of each cluster (+1).
    batch_size: int or None - Run batch learning if batch_size is None. Else,
      run online learning with specified batch size.
    observation_dim: int - dimension for observation vectors
    device: str - Your device. E.g., `cuda:0` or `cpu`.

  Returns:
    packed_rnn_input: (PackedSequence object) packed rnn input
    rnn_truth: ground truth
  &#34;&#34;&#34;
  num_clusters = len(seq_lengths)
  sorted_seq_lengths = np.sort(seq_lengths)[::-1]
  permute_index = np.argsort(seq_lengths)[::-1]

  if batch_size is None:
    rnn_input = np.zeros((sorted_seq_lengths[0],
                          num_clusters,
                          observation_dim))
    for i in range(num_clusters):
      rnn_input[1:sorted_seq_lengths[i], i,
                :] = sub_sequences[permute_index[i]]
    rnn_input = autograd.Variable(
        torch.from_numpy(rnn_input).float()).to(device)
    packed_rnn_input = torch.nn.utils.rnn.pack_padded_sequence(
        rnn_input, sorted_seq_lengths, batch_first=False)
  else:
    mini_batch = np.sort(np.random.choice(num_clusters, batch_size))
    rnn_input = np.zeros((sorted_seq_lengths[mini_batch[0]],
                          batch_size,
                          observation_dim))
    for i in range(batch_size):
      rnn_input[1:sorted_seq_lengths[mini_batch[i]],
                i, :] = sub_sequences[permute_index[mini_batch[i]]]
    rnn_input = autograd.Variable(
        torch.from_numpy(rnn_input).float()).to(device)
    packed_rnn_input = torch.nn.utils.rnn.pack_padded_sequence(
        rnn_input, sorted_seq_lengths[mini_batch], batch_first=False)
  # ground truth is the shifted input
  rnn_truth = rnn_input[1:, :, :]
  return packed_rnn_input, rnn_truth</code></pre>
</details>
</dd>
<dt id="uisrnn.utils.resize_sequence"><code class="name flex">
<span>def <span class="ident">resize_sequence</span></span>(<span>sequence, cluster_id, num_permutations=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Resize sequences for packing and batching.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sequence</code></strong></dt>
<dd>(real numpy matrix, size: seq_len*obs_size) - observed sequence</dd>
<dt><strong><code>cluster_id</code></strong></dt>
<dd>(numpy vector, size: seq_len) - cluster indicator sequence</dd>
<dt><strong><code>num_permutations</code></strong></dt>
<dd>int - Number of permutations per utterance sampled.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>sub_sequences</code></strong></dt>
<dd>A list of numpy array, with obsevation vector from the same
cluster in the same list.</dd>
<dt><strong><code>seq_lengths</code></strong></dt>
<dd>The length of each cluster (+1).</dd>
<dt><strong><code>bias</code></strong></dt>
<dd>Flipping coin head probability.</dd>
<dt><strong><code>bias_denominator</code></strong></dt>
<dd>The denominator of the bias, used for multiple calls to
fit().</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def resize_sequence(sequence, cluster_id, num_permutations=None):
  &#34;&#34;&#34;Resize sequences for packing and batching.

  Args:
    sequence: (real numpy matrix, size: seq_len*obs_size) - observed sequence
    cluster_id: (numpy vector, size: seq_len) - cluster indicator sequence
    num_permutations: int - Number of permutations per utterance sampled.

  Returns:
    sub_sequences: A list of numpy array, with obsevation vector from the same
      cluster in the same list.
    seq_lengths: The length of each cluster (+1).
    bias: Flipping coin head probability.
    bias_denominator: The denominator of the bias, used for multiple calls to
      fit().
  &#34;&#34;&#34;
  # merge sub-sequences that belong to a single cluster to a single sequence
  unique_id = np.unique(cluster_id)
  sub_sequences = []
  seq_lengths = []
  if num_permutations and num_permutations &gt; 1:
    for i in unique_id:
      idx_set = np.where(cluster_id == i)[0]
      sampled_idx_sets = sample_permuted_segments(idx_set, num_permutations)
      for j in range(num_permutations):
        sub_sequences.append(sequence[sampled_idx_sets[j], :])
        seq_lengths.append(len(idx_set) + 1)
  else:
    for i in unique_id:
      idx_set = np.where(cluster_id == i)
      sub_sequences.append(sequence[idx_set, :][0])
      seq_lengths.append(len(idx_set[0]) + 1)

  # compute bias
  transit_num = 0
  for entry in range(len(cluster_id) - 1):
    transit_num += (cluster_id[entry] != cluster_id[entry + 1])
  bias_denominator = len(cluster_id)
  bias = (transit_num + 1) / bias_denominator
  return sub_sequences, seq_lengths, bias, bias_denominator</code></pre>
</details>
</dd>
<dt id="uisrnn.utils.sample_permuted_segments"><code class="name flex">
<span>def <span class="ident">sample_permuted_segments</span></span>(<span>index_sequence, number_samples)</span>
</code></dt>
<dd>
<section class="desc"><p>Sample sequences with permuted blocks.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>index_sequence</code></strong></dt>
<dd>(integer array, size: L)
- subsequence index
For example, index_sequence = [1,2,6,10,11,12].</dd>
<dt><strong><code>number_samples</code></strong></dt>
<dd>(integer)
- number of subsampled block-preserving permuted sequences.
For example, number_samples = 5</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>sampled_index_sequences</code></strong></dt>
<dd>(a list of numpy arrays) - a list of subsampled
block-preserving permuted sequences. For example,</dd>
</dl>
<pre><code>sampled_index_sequences =
[[10,11,12,1,2,6],
 [6,1,2,10,11,12],
 [1,2,10,11,12,6],
 [6,1,2,10,11,12],
 [1,2,6,10,11,12]]
</code></pre>
<p>The length of "sampled_index_sequences" is "number_samples".</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def sample_permuted_segments(index_sequence, number_samples):
  &#34;&#34;&#34;Sample sequences with permuted blocks.

  Args:
    index_sequence: (integer array, size: L)
      - subsequence index
      For example, index_sequence = [1,2,6,10,11,12].
    number_samples: (integer)
      - number of subsampled block-preserving permuted sequences.
      For example, number_samples = 5

  Returns:
    sampled_index_sequences: (a list of numpy arrays) - a list of subsampled
      block-preserving permuted sequences. For example,
    ```
    sampled_index_sequences =
    [[10,11,12,1,2,6],
     [6,1,2,10,11,12],
     [1,2,10,11,12,6],
     [6,1,2,10,11,12],
     [1,2,6,10,11,12]]
    ```
      The length of &#34;sampled_index_sequences&#34; is &#34;number_samples&#34;.
  &#34;&#34;&#34;
  segments = []
  if len(index_sequence) == 1:
    segments.append(index_sequence)
  else:
    prev = 0
    for i in range(len(index_sequence) - 1):
      if index_sequence[i + 1] != index_sequence[i] + 1:
        segments.append(index_sequence[prev:(i + 1)])
        prev = i + 1
      if i + 1 == len(index_sequence) - 1:
        segments.append(index_sequence[prev:])
  # sample permutations
  sampled_index_sequences = []
  for _ in range(number_samples):
    segments_array = []
    permutation = np.random.permutation(len(segments))
    for permutation_item in permutation:
      segments_array.append(segments[permutation_item])
    sampled_index_sequences.append(np.concatenate(segments_array))
  return sampled_index_sequences</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="uisrnn.utils.Logger"><code class="flex name class">
<span>class <span class="ident">Logger</span></span>
</code></dt>
<dd>
<section class="desc"><p>A class for printing logging information to screen.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Logger:
  &#34;&#34;&#34;A class for printing logging information to screen.&#34;&#34;&#34;

  def __init__(self, verbosity):
    self._verbosity = verbosity

  def print(self, level, message):
    &#34;&#34;&#34;Print a message if level is not higher than verbosity.

    Args:
      level: the level of this message, smaller value means more important
      message: the message to be printed
    &#34;&#34;&#34;
    if level &lt;= self._verbosity:
      print(message)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="uisrnn.utils.Logger.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, verbosity)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize self.
See help(type(self)) for accurate signature.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, verbosity):
  self._verbosity = verbosity</code></pre>
</details>
</dd>
<dt id="uisrnn.utils.Logger.print"><code class="name flex">
<span>def <span class="ident">print</span></span>(<span>self, level, message)</span>
</code></dt>
<dd>
<section class="desc"><p>Print a message if level is not higher than verbosity.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>level</code></strong></dt>
<dd>the level of this message, smaller value means more important</dd>
<dt><strong><code>message</code></strong></dt>
<dd>the message to be printed</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def print(self, level, message):
  &#34;&#34;&#34;Print a message if level is not higher than verbosity.

  Args:
    level: the level of this message, smaller value means more important
    message: the message to be printed
  &#34;&#34;&#34;
  if level &lt;= self._verbosity:
    print(message)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="uisrnn" href="index.html">uisrnn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="uisrnn.utils.concatenate_training_data" href="#uisrnn.utils.concatenate_training_data">concatenate_training_data</a></code></li>
<li><code><a title="uisrnn.utils.enforce_cluster_id_uniqueness" href="#uisrnn.utils.enforce_cluster_id_uniqueness">enforce_cluster_id_uniqueness</a></code></li>
<li><code><a title="uisrnn.utils.generate_random_string" href="#uisrnn.utils.generate_random_string">generate_random_string</a></code></li>
<li><code><a title="uisrnn.utils.output_result" href="#uisrnn.utils.output_result">output_result</a></code></li>
<li><code><a title="uisrnn.utils.pack_sequence" href="#uisrnn.utils.pack_sequence">pack_sequence</a></code></li>
<li><code><a title="uisrnn.utils.resize_sequence" href="#uisrnn.utils.resize_sequence">resize_sequence</a></code></li>
<li><code><a title="uisrnn.utils.sample_permuted_segments" href="#uisrnn.utils.sample_permuted_segments">sample_permuted_segments</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="uisrnn.utils.Logger" href="#uisrnn.utils.Logger">Logger</a></code></h4>
<ul class="">
<li><code><a title="uisrnn.utils.Logger.__init__" href="#uisrnn.utils.Logger.__init__">__init__</a></code></li>
<li><code><a title="uisrnn.utils.Logger.print" href="#uisrnn.utils.Logger.print">print</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>